---
title: pattern recognition
date: 2023-10-20 00:00:00 +0800
categories: [Pattern Recognition]
tags: [theory]     # TAG names should always be lowercase
math: true
---


The goal of pattern recognition is to identify real physical objects based on sensed images. There are two approaches:

- Statistical approach: Bayes theory
- Syntactic approach: Automata theory

A generic pattern recognition system

1. learn the features of the objects to obtain the knowledge

   collect (representative) training data from sensor

   pre-process training data and extract features from training date system

2. recognize the different objects using the knowledge

   data from real-world captured by sensor

   pre-process and extract features same with training data

   compare features



## Automate theory

### What

Automate theory focuses on abstract models for different aspects of **computation**.

- Sequential computation
  - Finite memory: Finite state automata
  - Finite memory + Stack: Pushdown automata
  - Unrestricted: Turing machines
- Concurrent and reactive systems
  - Nonterminating (ω-words): Büchi automata
  - Nondeterministic (ω-trees): Büchi tree automata
- Rewriting systems
  - Terms: Tree automata over ranked trees
- Semi-structured data
  - XML documents: Tree automata over unranked trees

In theoretical computer science, automata theory is "the study of **mathematical properties** of **abstract** computing machines". More specifically,

- Expressivity

  What the model can and cannot do ?(class of language, computational problems)

- Closure properties

  closed under different operations

- Decidability and complexity

  Are the decision problems decidable? If yes, can they be solved in polynomial-time? (efficient algorithms)

### Why

Automata theory lays the theoretical foundations of various branches of computer science:

- Origin of computer science

  Turing machine

- Compiler design

  - Lexical analysis (Finite state automata)
  - Syntactical analysis (Pushdown automata)
  - Code selection (Tree automata)

- Foundations of model checking

  - Büchi automata

  - Rabin tree automata

- Foundations of Web data (XML document) processing

  Automata over unranked trees

- ...

Moreover, compared to specific programming languages, automata theory is more abstract and fundamental, thus **ease the mathematical reasoning**, but still **reflects the essence of computation**.

### How

Learn different types of structures and corresponding algorithms.

- Finite and infinite words

  Provided a fixed, finite, nonempty set $$ \Sigma $$, called *alphabet*. The elements of an alphabet are called *letters*. A finite, possibly empty sequence of letters is a *word*.

  Example:

  - regex (regular expression) on ASCII or Unicode
  - using a float to approximate a real number, since real numbers are infinite

  An infinite word, also called an $\omega$-word, is an infinite sequence of letters of $\Sigma$. We denote by $\Sigma^\omega$ the set of all $\omega$-word over $\Sigma$.

- Finite and infinite trees

  Ranked alphabet $\Sigma$ has a rank function $r:\Sigma\to\mathbb N$.

  Tree domain is a nonempty subset $D$ of $\mathbb N$ such that

  - if $x_i\in D$, then $x\in D$;
  - if $x_i\in D$, then $x_j\in D$ for any $j\le i$.

  A $\Sigma$-tree is a mapping $t:D\to\Sigma$ such that
  
$$
  \begin{align*}
  \forall x\in D,r(t(x))=\lvert\max\{i\mid x_i\in D\}\rvert.
  \end{align*}
  $$

  Example:

  ![finite ranked tree example](/assets/img/Test.assets/finite_ranked_tree_example.png)

  Here we have $f,g,h,x,y,z$ in tree domain. The rank of $y$ is $10$ instead of $01$ by the definition of $\Sigma$-tree and the ranks lower that $10$ all has its own letter.

  

  Unranked trees need only a mapping $t:D\to\Sigma$ (no rank constraint)

  $\omega$-trees if infinite

### Finite words

#### Definitions

- Alphabet  $\Sigma$: a finite set of symbols

- String: a finite sequence of symbols from a specific alphabet

  Empty String: $\phi$

- $\Sigma^*$: the set of all strings of $\Sigma$

- $\Sigma^+$: $\Sigma^*\backslash\{\phi\}$

- Language: any subset of $\Sigma^*$

#### Grammar

A grammar is a four-tuple

$$
\begin{align*}
G&=(V_N,V_T,P,S)
\end{align*}
$$

respectively a set of non-terminals (variables), a set of terminals (constants), a set of productions or rewriting rules, and the start of root symbol.

There are four types of Grammars:

- unrestricted grammar (G0)

  $P:V^+\to V^*$

- context-sensitive grammar (G1)

  $P:V^+\to V^*$ and $\lvert P(v)\rvert\ge\lvert v\rvert$

  another definition goes
  
$$
  \begin{align*}
  &&\alpha_1A\alpha_2&\to\alpha_1\beta\alpha_2\\
  \text{where }&&\alpha_1,\alpha_2&\in V^*\\
  &&\beta&\in V^+\\
  &&A&\in V_N
  \end{align*}
  $$


- context-free grammar (G2)

  $P:V_N\to V^+$

  another definition goes
  
$$
  \begin{align*}
  &&\alpha_1A\alpha_2&\to\alpha_1\beta\alpha_2\\
  \text{where }&&\alpha_1,\alpha_2&\in V^*\\
  &&\beta&\in V^+\\
  &&A&\in V_N
  \end{align*}
  $$


- regular grammar (G3)
  
$$
  \begin{align*}
  &&A&\to aB\\
  &&A&\to a\\
  \text{where }&&A,B&\in V_N\\
  &&a&\in V_T
  \end{align*}
  $$

  or
  
$$
  \begin{align*}
  &&A&\to Ba\\
  &&A&\to a\\
  \text{where }&&A,B&\in V_N\\
  &&a&\in V_T
  \end{align*}
  $$



$$
\begin{align*}
L(G3)\subset L(G2)\subset L(G1)\subset L(G0)\\
\underrightarrow{\text{increasing representational capability}}\\
\underrightarrow{\text{increasing recognition difficulty}}\\
\underrightarrow{\text{decreasing production constraints}}\\
\end{align*}
$$


#### State diagram

A <abbr title="deterministic finite automata">DFA</abbr> can be denoted by a 5-tuple $(Q,\Sigma,P,S,F)$

- a set of states $Q$
- a set of symbols $\Sigma$
- the transition function $P:Q\times\Sigma\to Q$
- the initial state $S\in Q$
- a set of final state $F\subset Q$

| DFA                                                          | NDFA                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| deterministic finite automata                                | non-deterministic finite automata                            |
| The transition takes place from a state to a single particular state for each input symbol. | For each input symbol, the transition can be to multiple next states. |
| no empty string transitions                                  | permit empty string transitions                              |
| allow backtracking                                           | impossible to backtrack                                      |
| more space                                                   | less space                                                   |
| no randomness                                                | more randomness                                              |
| $P:Q\times\Sigma\to Q$                                       | $P:Q\times\Sigma\to 2^Q$                                     |

The change from NDFA to DFA can be achieved by transition table. Specifically, the multiple next states in NDFA can be merged into one state in DFA.



### Syntax-directed recognition

Methods:

- matching: library

- parsing: pre-specified grammar

  top-down approach

  bottom-up approach

For the general patterns, we follow 3 steps:

1. create the grammar of patterns with automata structure (words or trees) and corresponding rules
2. represent the grammar graphically to produce the so-called state diagram in the form of DFA
3. recognize the representation of grammar







## Bayes statistical decision theory

A set of patterns $\Omega$ to be classified is composed of patterns of two  mutually exclusive and complete classes, $\Omega_{-1}$ and $\Omega_1$

$$
\begin{align*}
\Omega_{-1}\cup\Omega_1&=\Omega\\
\Omega_{-1}\cap\Omega_1&=\varnothing\\
\end{align*}
$$


### Pattern space

To establish a mapping from pattern space to feature space, the pattern space ought to be measurable, thus need a $\sigma$-field $F$ and corresponding measure function $P$ over the pattern space.

Besides, pattern space is diverse and not uniform


$$
\begin{align*}
(\Omega,F,P)\underset{\text{measurable mapping}}{\xlongequal{\text{feature extraction}}}(\mathbb R^n,\mathbb B^n)
\end{align*}
$$


The pattern space is mapped to the feature space by feature vector, where

- $\Omega$ is the pattern space (basic events)
- $F$ is the $\sigma$-field constructed by the subsets of $\Omega$ (complex events)
- $P$ is a probability measure defined over F and determined by priori probabilities, satisfying $P(\Omega)=1$
- $\mathbb R^n$ is an n-dimensional feature space
- $\mathbb B^n$ is a Borel σ- field in $\mathbb R^n$ (where random variables $X(\omega)$ are defined)

### Classification

Based on these concepts of space, we proceed to other definitions in binary classification problems:

- Decision function

  $d(\omega):(\Omega,F,P)\to\{-1,1\}$;

- Borel measurable function

  $c(x):(\mathbb R^n,\mathbb B^n)\to\{-1,1\}$.

Indeed, decision function is merely the composite of random feature vector $X(\omega)$ and the Borel measurable function $c(x)$

$$
\begin{align*}
d(\omega)&=c(x)=c(X(\omega))\\
(\Omega,F,P)&\overset{X}{\to}(\mathbb R^n,\mathbb B^n)\overset{c}{\to}\{-1,1\}
\end{align*}
$$


- Class function

  $J(\omega):(\Omega,F,P)\to\{-1,1\}$, which represents the nature of pattern $J(\omega)=i,\omega\in\Omega_i$

- Loss function is based on those of decision function $d$ and class function $J$, referred to the loss cased by misclassification (and correct classification)

  $L(d(\omega),J(\omega))$

  Average loss $R$ is the integral of loss function over feature space according to the probability.
  
$$
  \begin{align*}
  R&=\mathbb EL(d(\omega),J(\omega))\\
  &=\mathbb EL(c(x),J_x(x))\\
  &=\sum_{j=-1,1}\int_{\mathbb B^n}L(c(x),J_X(x))p(x,j)\,\mathrm dx\\
  &=\sum_{j=-1,1}\int_{\mathbb B^n}L(c(x),J_X(x))p(x|j)p(j)\,\mathrm dx,\\
  \end{align*}
  $$

  where $p(x|j)$ is the conditional probability (likelihood) and $p(j)$ is the priori probability.

  Minimizing $R$ is equivalent to finding optimal $c$ under certain $L$. Typically, there are two criteria: minimum average-loss and minimum error-probability.



## Machine learning

### Learning models

- Linear
  
$$
  \begin{align*}
  \hat y&=\pmb w^\top\pmb\phi(x)
  \end{align*}
  $$

  

Polemical function

$$
  \begin{align*}
  \pmb\phi(x)=[1,x,x^2,\ldots,x^n]^\top
  \end{align*}
$$


- Core
  
$$
  \begin{align*}
  \hat y&=\sum_{i=1}^n\theta_iK(x,x_i)
  \end{align*}
  $$

  Gaussian kernel function
  
$$
  \begin{align*}
  K(x,z)&=\exp\left(-{\lVert x-z\rVert_2^2\over2h^2}\right)
  \end{align*}
  $$


- Hierarchical
  
$$
  \begin{align*}
  \hat y&=\pmb w^\top\pmb\psi(x;\pmb\beta)
  \end{align*}
  $$

  S function
  
$$
  \begin{align*}
  \pmb\psi&=\frac1{1+\exp(-\pmb\beta^\top\pmb\phi(x))}
  \end{align*}
  $$


### Loss


$$
\begin{align*}
\mathcal L(y,x;\theta)&=\frac12(\hat y-y)^2\\
\theta^\star&=\underset{\theta}{\arg\min}\,\mathcal L
\end{align*}
$$




### Learning methods

Least squares learning for linear model

Gradient descent for massive scale model

Constraint under $\ell_p$ norm

## Pattern acquisition

- Acquisition: obtain data
- Preprocessing: modify data
- Feature extraction: represent data
- Classification: classify data
- Post-processing: improve result

### The process of obtaining a digital image from a real object

- Sensing light

  spectrum and the course of sensing

  spectral sensitivity in different frequency ranges of light

- Image devices (and human eyes)

  tiny solid state cells convert light energy into electrical charge

  rod cells and cone cells measure the luminance and color, respectively

### Types of images

An **analog image** has continuous coordinates and intensify of infinite precision; in contrast, a **digit image** has discrete coordinates and intensify of finite precision. 

A **binary image** is a special case of digital image with all pixel value of 0 or 1. A **labeled image** is a digital image whose pixel values are symbols.

Depending on the **ranges of frequency and number of channels**, image can be classified as gray-scale image, color image, and hyperspectral image.

### Quantization

The *nominal resolution* of a CCD sensor is the size of the scene element that images to a single pixel on the image plane.

A quantizer is an Analog-to-Digital device which converts a continuous input signal $u$ to one of a set of discrete levels called reconstruction levels $r_k$.

Lloyd-Max quantizer deems that the luminance of light follows a certain distribution. It minimizes the expected loss between digital image and analog image.

$$
\begin{align*}
\min\mathbb E[(u-u^\star)]
\end{align*}
$$

special cases

- Uniform distribution
- Non-uniform distribution



Quantization is equivalent to a low-pass filter that filters out the high-frequency components in images.

Spatial quantization and intensity quantization

### Formats of digital images

Standard formats have been developed so that different hardware and software can share data.

Common formats: run-coded binary, portable bit map (PBM/PGM, PPM), graphics interchange (GIF), tag image (TIF), for still photo (JPEG), postscript (EPS), and for video (MPEG). They are often composed of two parts:

- raw image data
- a header contains the meta information of image



## Binary image analysis in preprocessing phase

A binary image B can be obtained from a gray-scale or color image through an operation that
- selects a subset of the image pixels as foreground pixels (pixels of interest in an image analysis task)

- leaving the rest as background pixels to be ignored

The selection operation can be

- simple: thresholding operator
- complex: classification algorithm

### Mask

Apply mask to images using 2D-convolution,

$$
\begin{align*}
(f*g)(t_1,t_2)&=\iint f(\tau_1,\tau_2)g(t_1-\tau_1,t_2-\tau_2)\,\mathrm d\tau_1\,\mathrm d\tau_2\\
(f*g)[m_1,m_2]&=\sum_{n_1,n_2} f[n_1,n_2]g[m_1-n_1,m_2-n_2]
\end{align*}
$$

where $f$ is the image and $g$ is the mask. A mask is set of pixel positions and corresponding values called weights.

### Example

For instance, image can be smoothed by convolving itself with a specific mask.

$$
\begin{align*}
\text{image}\otimes\begin{bmatrix}
1&2&1\\
2&4&2\\
1&2&1\\
\end{bmatrix}=\text{smoothed image}
\end{align*}
$$

There are several important operations in convolution:

- padding: add some virtual rows and columns to the input image around the edges to keep the size consistent
- Normalization: divide the value by the sum of weights in the mask to keep the brightness consistent
- ...

#### Counting objects

Counting objects by corner mask $E$ (and $I$), both of which are 2x2 masks with three 0s and one 1-pixel (and the other way around).

$$
\begin{align*}
\#(\text{foreground objects})&\approx\frac{\lvert I-E\rvert}4
\end{align*}
$$


#### Connected components labeling

- for small image, a recursive algorithm is applied, which works on one component at a time
  1. negate the binary image, which means pixels of value 1 become those of value -1, to distinguish unprocessed pixels from those of component label 1
  2. find a pixel of value -1 and assign it a new label value
  3. recursively search its neighbors of value -1 and assign it the same label value
  4. back to 2 until no pixel of value -1
- for large image, a memory-efficient algorithm is applied, which works on only two rows of the image at a time
  1. record equivalences and assign temporary labels
  2. replace each temporary label by the label of its equivalence class
  3. loop

### Binary image morphology

The word morphology refers to form and structure. Specifically, in pattern recognition, it can be used to refer to the shape of a region.

Basic operations:

- Dilation: enlarge a region
  
$$
  \begin{align*}
  B\oplus S=\bigcup_{b\in B}S_b
  \end{align*}
  $$


- Erosion: make a region smaller
  
$$
  \begin{align*}
  B\ominus S=\{b\mid b+s\in B,\forall s\in S\}
  \end{align*}
  $$


- Closing: close up internal holes in a region and eliminate bays along the boundary
  
$$
  \begin{align*}
  B\bullet S=(B\oplus S)\ominus  S
  \end{align*}
  $$


- Opening: get rid of small portions of the region that just out from the boundary into the background region
  
$$
  \begin{align*}
  B\circ S=(B\ominus S)\oplus S
  \end{align*}
  $$

  

![applications of binary morphology](/assets/img/Test.assets/applications_of_binary_morphology.png)

### Changing gray-scale image to binary image

The gray-level histogram H(z) is the discrete graph plotted with the number of pixels at gray level z.

$$
\begin{align*}
H(z)=\#(f(x,y)=z,\forall x,y)
\end{align*}
$$

Binary images can be obtained from gray-scale images by thresholding operations. Given the distribution of gray tones in a given image, certain gray-tone values can be chosen as threshold values that separate the pixels into groups.

### Image enhancement

Image enhancement operations improve the detectability of important image details or objects.

#### Gray-level mapping

Mapping function is a piecewise smooth function applied pixel-wise on the image.

- Gamma correction $f(x;\beta)=x^{1/\beta}$ boosts dark pixels much more that bright ones.
- Reverse $f(x)=\max-x$.

An important metrics of image's quality is its histogram of brightness, thus it is reasonable to apply proper function to adjust the distribution of image's histogram and to obtain better visual effects and analytical performances.

- Histogram equalization (enhance contrast)

  transform probability distribution function of $u$ to uniform distribution one $P_s$, which satisfying
  
$$
  \begin{align*}
  P_s(s)\,\mathrm ds&=P_u(u)\,\mathrm du.
  \end{align*}
  $$

  Since $P_s(s)=1$, we have
  
$$
  \begin{align*}
  \mathrm ds&=P_u(u)\,\mathrm du\\
  s(u)&=\int_0^uP_u(r)\,\mathrm dr
  \end{align*}
  $$

  

Instead of uniform distribution, $P_s(s)$ can be any distribution so long as to $s(u)=f^{-1}(g(u))$, where $f,g$ are the cumulative distribution function of $u,s$ respectively.

  In discrete case, replace $\int_0^x P\mathrm dx$ with $\sum_{j=0}^xn_j/n$ ($P=n_j/n$).

#### Removal of small image regions

remove salt-and-pepper noise by 8-neighbor mask or 4-neighbor mask.

Specifically, The pepper noise can be removed by detecting its neighbors.

$$
\begin{align*}
\text{8-neighbor}\begin{bmatrix}
1&1&1\\
1&0&1\\
1&1&1\\
\end{bmatrix}\to
\begin{bmatrix}
1&1&1\\
1&1&1\\
1&1&1\\
\end{bmatrix}\\
\text{4-neighbor}\begin{bmatrix}
&1&\\
1&0&1\\
&1&\\
\end{bmatrix}\to
\begin{bmatrix}
&1&\\
1&1&1\\
&1&\\
\end{bmatrix}\\
\end{align*}
$$



#### Image smoothing

The value of a pixel is determined by itself and its neighbors, which is their weighted average. Different weights represent different filters.

- Box filter

  Average of the neighborhood of image

- Gaussian filter
  
$$
  \begin{align*}
  w(x,y)&=\frac1{\sqrt{2\pi}\sigma}\exp\left(-{(x-x_c)^2+(y-y_c)^2\over2\sigma^2}\right)
  \end{align*}
  $$


- Averaging

  Average $N$ different noisy image.
  
$$
  \begin{align*}
  \bar f(x,y)=\frac1N\sum_{i=1}^Nf_i(x,y)
  \end{align*}
  $$


- Median filter

  The value of a pixel is replaced by median value of the neighborhood.

  Cost more time but obtain better performance.





